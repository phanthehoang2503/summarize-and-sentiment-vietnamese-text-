{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import logging\n",
    "from typing import Set\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).parent.parent if '__file__' in locals() else Path('../')\n",
    "RAW_DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "PROCESSED_DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "STOPWORDS_FILE = PROJECT_ROOT / \"data\" / \"vietnamese-stopwords.txt\"\n",
    "\n",
    "PROCESSED_DATA_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DEFAULT_ENCODING = \"utf-8\"\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "602c06f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_stopwords\u001b[39m(stopwords_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mSet\u001b[49m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(stopwords_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Set' is not defined"
     ]
    }
   ],
   "source": [
    "def load_stopwords(stopwords_path: str) -> Set[str]:\n",
    "    try:\n",
    "        with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "            stopwords = set(word.strip().lower() for word in f.readlines() if word.strip())\n",
    "        return stopwords\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Stopwords file not found: {stopwords_path}\")\n",
    "        return set()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading stopwords: {e}\")\n",
    "        return set()\n",
    "\n",
    "def remove_emojis(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U00002702-\\U000027B0\"\n",
    "        \"\\U000024C2-\\U0001F251\"\n",
    "        \"\\U0001f926-\\U0001f937\"\n",
    "        \"\\U00010000-\\U0010ffff\"\n",
    "        \"\\u2640-\\u2642\"\n",
    "        \"\\u2600-\\u2B55\"\n",
    "        \"\\u200d\"\n",
    "        \"\\u23cf\"\n",
    "        \"\\u23e9\"\n",
    "        \"\\u231a\"\n",
    "        \"\\ufe0f\"  # dingbats\n",
    "        \"\\u3030\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    emoticon_pattern = re.compile(\n",
    "        r'[:;=8][\\-o\\*\\']?[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\]|[\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\][\\-o\\*\\']?[:;=8]')\n",
    "    text = emoticon_pattern.sub(r'', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_stopwords(text: str, stopwords: Set[str]) -> str:\n",
    "    if pd.isna(text) or not stopwords:\n",
    "        return text\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def clean_sentiment_text(text: str, stopwords: Set[str]) -> str:\n",
    "    pattern = r'(.)\\1{2,}'\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    text = remove_emojis(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[\"\"''\"\\']+', '', text)\n",
    "    text = remove_stopwords(text, stopwords)\n",
    "    text = re.sub(pattern, r'\\1\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_summarization_text(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[.]{3,}', '...', text)\n",
    "    text = re.sub(r'[!]{2,}', '!', text)\n",
    "    text = re.sub(r'[?]{2,}', '?', text)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "stopwords = load_stopwords(str(STOPWORDS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ce863b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cảm xúc: 31460 dòng\n",
      "Tóm tắt: 102681 dòng\n",
      "Tóm tắt: 102681 dòng\n"
     ]
    }
   ],
   "source": [
    "def preprocess_summarization(sample_fraction=0.5):\n",
    "    print(\"Processing summarization dataset...\")\n",
    "    \n",
    "    file_path = RAW_DATA_DIR / \"data_summary.csv\"\n",
    "    df = pd.read_csv(file_path, encoding=DEFAULT_ENCODING)\n",
    "    df = df[[\"Text\", \"Summary\"]]\n",
    "    \n",
    "    df_clean = df.dropna(subset=[\"Text\", \"Summary\"])\n",
    "    print(f\"Dropped {len(df) - len(df_clean)} rows with missing values\")\n",
    "    \n",
    "    if sample_fraction < 1.0:\n",
    "        df_clean = df_clean.sample(frac=sample_fraction, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "        print(f\"Sampled {sample_fraction:.1%} of data\")\n",
    "    \n",
    "    print(\"Cleaning Text and Summary columns...\")\n",
    "    df_clean['Text'] = df_clean['Text'].apply(clean_summarization_text)\n",
    "    df_clean['Summary'] = df_clean['Summary'].apply(clean_summarization_text)\n",
    "    \n",
    "    initial_count = len(df_clean)\n",
    "    df_processed = df_clean[\n",
    "        (df_clean['Text'].str.strip() != '') &\n",
    "        (df_clean['Summary'].str.strip() != '')\n",
    "    ].reset_index(drop=True)\n",
    "    \n",
    "    removed_count = initial_count - len(df_processed)\n",
    "    if removed_count > 0:\n",
    "        print(f\"Removed {removed_count} rows with empty text after cleaning\")\n",
    "    \n",
    "    output_file = PROCESSED_DATA_DIR / \"summary_clean.csv\"\n",
    "    df_processed.to_csv(output_file, index=False, encoding=DEFAULT_ENCODING)\n",
    "    \n",
    "    print(f\"Final summarization dataset shape: {df_processed.shape}\")\n",
    "    return df_processed\n",
    "\n",
    "summary_df = preprocess_summarization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0662468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 'Áo bao đẹp ạ!!' -> 'áo bao đẹp'\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentiment(sample_fraction=1.0):\n",
    "    print(\"Processing sentiment dataset...\")\n",
    "    \n",
    "    file_path = RAW_DATA_DIR / \"data_sentiment.csv\"\n",
    "    df = pd.read_csv(file_path, encoding=DEFAULT_ENCODING)\n",
    "    \n",
    "    df_clean = df.dropna(subset=[\"comment\", \"label\"])\n",
    "    print(f\"Dropped {len(df) - len(df_clean)} rows with missing values\")\n",
    "    \n",
    "    if sample_fraction < 1.0:\n",
    "        df_clean = df_clean.sample(frac=sample_fraction, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "        print(f\"Sampled {sample_fraction:.1%} of data\")\n",
    "    \n",
    "    print(\"Cleaning comment column...\")\n",
    "    df_clean['comment'] = df_clean['comment'].apply(lambda x: clean_sentiment_text(x, stopwords))\n",
    "    \n",
    "    initial_count = len(df_clean)\n",
    "    df_processed = df_clean[df_clean['comment'].str.strip() != ''].reset_index(drop=True)\n",
    "    df_processed = df_processed[df_processed['comment'].str.len() >= 10].reset_index(drop=True)\n",
    "    \n",
    "    removed_count = initial_count - len(df_processed)\n",
    "    if removed_count > 0:\n",
    "        print(f\"Removed {removed_count} rows with empty/short comments after cleaning\")\n",
    "    \n",
    "    output_file = PROCESSED_DATA_DIR / \"reviews_clean.csv\"\n",
    "    df_processed.to_csv(output_file, index=False, encoding=DEFAULT_ENCODING)\n",
    "    \n",
    "    print(f\"Final sentiment dataset shape: {df_processed.shape}\")\n",
    "    return df_processed\n",
    "\n",
    "sentiment_df = preprocess_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a30a280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu phân tích cảm xúc: 22479 records\n",
      "Dữ liệu tóm tắt: 102393 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Data preprocessing completed successfully!\")\n",
    "print(f\"Summarization data: {summary_df.shape[0]} records\")\n",
    "print(f\"Sentiment data: {sentiment_df.shape[0]} records\")\n",
    "\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(sentiment_df['label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
