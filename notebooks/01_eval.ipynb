{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T04:31:45.249218Z",
     "start_time": "2025-09-06T04:31:43.604069Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from: d:\\Project\\MajorProject\\params.yaml\n",
      "\n",
      "Configuration Validation:\n",
      "Project root: d:\\Project\\MajorProject\n",
      "Data directory: d:\\Project\\MajorProject\\data\n",
      "Models directory: d:\\Project\\MajorProject\\models\n",
      "\n",
      "File Check:\n",
      "raw: Found\n",
      "summary_clean.csv: Found\n",
      "reviews_clean.csv: Found\n",
      "vietnamese-stopwords.txt: Found\n",
      "\n",
      "Configuration validation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Clear any cached modules\n",
    "if 'config' in sys.modules:\n",
    "    del sys.modules['config']\n",
    "\n",
    "# Setup project path\n",
    "current_path = Path.cwd()\n",
    "project_root = current_path.parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load and validate configuration\n",
    "from config import config\n",
    "print(f\"Configuration loaded from: {config.config_path}\")\n",
    "\n",
    "# Validate configuration paths\n",
    "print(\"\\nConfiguration Validation:\")\n",
    "print(f\"Project root: {config.project_root}\")\n",
    "print(f\"Data directory: {config.data_dir}\")\n",
    "print(f\"Models directory: {config.models_dir}\")\n",
    "\n",
    "# Check if required files exist\n",
    "required_files = [\n",
    "    config.raw_data_dir,\n",
    "    config.summarization_data,\n",
    "    config.sentiment_data,\n",
    "    config.stopwords_file\n",
    "]\n",
    "\n",
    "print(\"\\nFile Check:\")\n",
    "for file_path in required_files:\n",
    "    if file_path.exists():\n",
    "        print(f\"{file_path.name}: Found\")\n",
    "    else:\n",
    "        print(f\"{file_path.name}: Missing\")\n",
    "\n",
    "print(\"\\nConfiguration validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134f5e6a248c38b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T04:31:56.661318Z",
     "start_time": "2025-09-06T04:31:47.299375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Summarization Dataset:\n",
      "------------------------------\n",
      "Successfully loaded: 51,292 samples\n",
      "Dataset shape: (51292, 2)\n",
      "Columns: ['Text', 'Summary']\n",
      "\n",
      "Data Quality Checks:\n",
      "All required columns present\n",
      "No null values found\n",
      "No empty Text entries\n",
      "No empty Summary entries\n",
      "\n",
      "2. Sentiment Dataset:\n",
      "------------------------------\n",
      "Successfully loaded: 12,183 samples\n",
      "Dataset shape: (12183, 4)\n",
      "Columns: ['comment', 'label', 'rate', 'Unnamed: 3']\n",
      "\n",
      "Data Quality Checks:\n",
      "All required columns present\n",
      "Warning - Null values found:\n",
      "  - Unnamed: 3: 12169 nulls (99.9%)\n",
      "\n",
      "Label Distribution:\n",
      "  - POS: 4,061 samples (33.3%)\n",
      "  - NEG: 4,061 samples (33.3%)\n",
      "  - NEU: 4,061 samples (33.3%)\n"
     ]
    }
   ],
   "source": [
    "# Data Ingestion & Validation\n",
    "# Load summarization dataset\n",
    "print(\"\\n1. Summarization Dataset:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    summ_data = load_dataset('csv', data_files=str(config.summarization_data))\n",
    "    df_summ = summ_data['train'].to_pandas()\n",
    "    print(f\"Successfully loaded: {len(df_summ):,} samples\")\n",
    "    print(f\"Dataset shape: {df_summ.shape}\")\n",
    "    print(f\"Columns: {list(df_summ.columns)}\")\n",
    "    \n",
    "    # Data validation\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['Text', 'Summary']\n",
    "    missing_cols = [col for col in required_cols if col not in df_summ.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        print(f\"Available columns: {list(df_summ.columns)}\")\n",
    "    else:\n",
    "        print(f\"All required columns present\")\n",
    "    \n",
    "    # Check for null values\n",
    "    null_counts = df_summ.isnull().sum()\n",
    "    if null_counts.any():\n",
    "        print(f\"Warning - Null values found:\")\n",
    "        for col, count in null_counts[null_counts > 0].items():\n",
    "            print(f\"  - {col}: {count} nulls ({count/len(df_summ)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"No null values found\")\n",
    "    \n",
    "    # Check for empty strings\n",
    "    if 'Text' in df_summ.columns and 'Summary' in df_summ.columns:\n",
    "        empty_content = (df_summ['Text'].str.strip() == '').sum()\n",
    "        empty_summary = (df_summ['Summary'].str.strip() == '').sum()\n",
    "        \n",
    "        if empty_content > 0:\n",
    "            print(f\"Warning - Empty Text entries: {empty_content}\")\n",
    "        else:\n",
    "            print(f\"No empty Text entries\")\n",
    "            \n",
    "        if empty_summary > 0:\n",
    "            print(f\"Warning - Empty Summary entries: {empty_summary}\")\n",
    "        else:\n",
    "            print(f\"No empty Summary entries\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load summarization dataset: {e}\")\n",
    "    df_summ = None\n",
    "\n",
    "# Load sentiment dataset\n",
    "print(\"\\n2. Sentiment Dataset:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "try:\n",
    "    sent_data = load_dataset('csv', data_files=str(config.sentiment_data))\n",
    "    df_sent = sent_data['train'].to_pandas()\n",
    "    print(f\"Successfully loaded: {len(df_sent):,} samples\")\n",
    "    print(f\"Dataset shape: {df_sent.shape}\")\n",
    "    print(f\"Columns: {list(df_sent.columns)}\")\n",
    "    \n",
    "    # Data validation\n",
    "    print(f\"\\nData Quality Checks:\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['comment', 'label']\n",
    "    missing_cols = [col for col in required_cols if col not in df_sent.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"All required columns present\")\n",
    "    \n",
    "    # Check for null values\n",
    "    null_counts = df_sent.isnull().sum()\n",
    "    if null_counts.any():\n",
    "        print(f\"Warning - Null values found:\")\n",
    "        for col, count in null_counts[null_counts > 0].items():\n",
    "            print(f\"  - {col}: {count} nulls ({count/len(df_sent)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"No null values found\")\n",
    "    \n",
    "    # Check label distribution\n",
    "    if 'label' in df_sent.columns:\n",
    "        label_counts = df_sent['label'].value_counts()\n",
    "        print(f\"\\nLabel Distribution:\")\n",
    "        for label, count in label_counts.items():\n",
    "            percentage = (count / len(df_sent)) * 100\n",
    "            print(f\"  - {label}: {count:,} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Failed to load sentiment dataset: {e}\")\n",
    "    df_sent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34e018f59527dd7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T04:32:07.655354Z",
     "start_time": "2025-09-06T04:32:05.955098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Summarization Dataset Exploration:\n",
      "----------------------------------------\n",
      "Text Length Statistics (Characters):\n",
      "  Content - Mean: 3229, Median: 2676\n",
      "  Summary - Mean: 140, Median: 141\n",
      "\n",
      "Word Count Statistics:\n",
      "  Content - Mean: 704.5, Median: 581\n",
      "  Summary - Mean: 30.5, Median: 31\n",
      "\n",
      "Compression Analysis:\n",
      "  Average compression ratio: 0.104\n",
      "  Median compression ratio: 0.051\n",
      "  Summary is 10.4% of original length\n",
      "\n",
      "Sample Entry:\n",
      "  Content (1016 words): Tập đoàn T&T Group và tập đoàn năng lượng Ørsted (Đan Mạch) vừa ký hợp tác trong lĩnh vực điện gió ngoài khơi tại Việt Nam. Theo đó, tổng công suất dự...\n",
      "  Summary (29 words): Doanh nghiệp vừa ký kết hợp tác với hai đối tác Bỉ và Đan Mạch nhằm phát triển các dự án về năng lượng tái tạo tại Việt Nam.\n",
      "  Compression: 2.85%\n",
      "\n",
      "2. Sentiment Dataset Exploration:\n",
      "----------------------------------------\n",
      "Text Length Statistics:\n",
      "  Characters - Mean: 34, Median: 26\n",
      "  Words - Mean: 7.8, Median: 6\n",
      "\n",
      "Label Distribution Analysis:\n",
      "  POS: 17,789 samples (64.0%) - Avg length: 8.2 words\n",
      "    Sample: Áo bao đẹp ạ!!...\n",
      "  NEG: 5,957 samples (21.4%) - Avg length: 7.4 words\n",
      "    Sample: 2day ao khong giong trong....\n",
      "  NEU: 4,061 samples (14.6%) - Avg length: 6.7 words\n",
      "    Sample: giá sản phẩm tạm ổn đc gọi đẹp lắm....\n",
      "\n",
      "Data Quality Metrics:\n",
      "  Very short comments (≤3 words): 4387 (15.8%)\n",
      "  Very long comments (≥100 words): 0 (0.0%)\n",
      "Text Length Statistics (Characters):\n",
      "  Content - Mean: 3229, Median: 2676\n",
      "  Summary - Mean: 140, Median: 141\n",
      "\n",
      "Word Count Statistics:\n",
      "  Content - Mean: 704.5, Median: 581\n",
      "  Summary - Mean: 30.5, Median: 31\n",
      "\n",
      "Compression Analysis:\n",
      "  Average compression ratio: 0.104\n",
      "  Median compression ratio: 0.051\n",
      "  Summary is 10.4% of original length\n",
      "\n",
      "Sample Entry:\n",
      "  Content (1016 words): Tập đoàn T&T Group và tập đoàn năng lượng Ørsted (Đan Mạch) vừa ký hợp tác trong lĩnh vực điện gió ngoài khơi tại Việt Nam. Theo đó, tổng công suất dự...\n",
      "  Summary (29 words): Doanh nghiệp vừa ký kết hợp tác với hai đối tác Bỉ và Đan Mạch nhằm phát triển các dự án về năng lượng tái tạo tại Việt Nam.\n",
      "  Compression: 2.85%\n",
      "\n",
      "2. Sentiment Dataset Exploration:\n",
      "----------------------------------------\n",
      "Text Length Statistics:\n",
      "  Characters - Mean: 34, Median: 26\n",
      "  Words - Mean: 7.8, Median: 6\n",
      "\n",
      "Label Distribution Analysis:\n",
      "  POS: 17,789 samples (64.0%) - Avg length: 8.2 words\n",
      "    Sample: Áo bao đẹp ạ!!...\n",
      "  NEG: 5,957 samples (21.4%) - Avg length: 7.4 words\n",
      "    Sample: 2day ao khong giong trong....\n",
      "  NEU: 4,061 samples (14.6%) - Avg length: 6.7 words\n",
      "    Sample: giá sản phẩm tạm ổn đc gọi đẹp lắm....\n",
      "\n",
      "Data Quality Metrics:\n",
      "  Very short comments (≤3 words): 4387 (15.8%)\n",
      "  Very long comments (≥100 words): 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Data Exploration & Analysis\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "\n",
    "# Summarization Dataset Exploration\n",
    "if df_summ is not None:\n",
    "    print(\"\\n1. Summarization Dataset Exploration:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'Text' in df_summ.columns and 'Summary' in df_summ.columns:\n",
    "        # Calculate text statistics\n",
    "        df_summ['content_length'] = df_summ['Text'].str.len()\n",
    "        df_summ['summary_length'] = df_summ['Summary'].str.len()\n",
    "        df_summ['content_words'] = df_summ['Text'].str.split().str.len()\n",
    "        df_summ['summary_words'] = df_summ['Summary'].str.split().str.len()\n",
    "        df_summ['compression_ratio'] = df_summ['summary_words'] / df_summ['content_words']\n",
    "        \n",
    "        print(f\"Text Length Statistics (Characters):\")\n",
    "        print(f\"  Content - Mean: {df_summ['content_length'].mean():.0f}, Median: {df_summ['content_length'].median():.0f}\")\n",
    "        print(f\"  Summary - Mean: {df_summ['summary_length'].mean():.0f}, Median: {df_summ['summary_length'].median():.0f}\")\n",
    "        \n",
    "        print(f\"\\nWord Count Statistics:\")\n",
    "        print(f\"  Content - Mean: {df_summ['content_words'].mean():.1f}, Median: {df_summ['content_words'].median():.0f}\")\n",
    "        print(f\"  Summary - Mean: {df_summ['summary_words'].mean():.1f}, Median: {df_summ['summary_words'].median():.0f}\")\n",
    "        \n",
    "        print(f\"\\nCompression Analysis:\")\n",
    "        print(f\"  Average compression ratio: {df_summ['compression_ratio'].mean():.3f}\")\n",
    "        print(f\"  Median compression ratio: {df_summ['compression_ratio'].median():.3f}\")\n",
    "        print(f\"  Summary is {df_summ['compression_ratio'].mean()*100:.1f}% of original length\")\n",
    "        \n",
    "        # Sample data preview\n",
    "        print(f\"\\nSample Entry:\")\n",
    "        sample = df_summ.iloc[0]\n",
    "        print(f\"  Content ({sample['content_words']} words): {sample['Text'][:150]}...\")\n",
    "        print(f\"  Summary ({sample['summary_words']} words): {sample['Summary']}\")\n",
    "        print(f\"  Compression: {sample['compression_ratio']:.2%}\")\n",
    "    else:\n",
    "        print(f\"Expected Text and Summary columns not found. Available columns: {list(df_summ.columns)}\")\n",
    "\n",
    "# Sentiment Dataset Exploration  \n",
    "if df_sent is not None:\n",
    "    print(f\"\\n2. Sentiment Dataset Exploration:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'comment' in df_sent.columns:\n",
    "        # Calculate text statistics\n",
    "        df_sent['comment_length'] = df_sent['comment'].str.len()\n",
    "        df_sent['comment_words'] = df_sent['comment'].str.split().str.len()\n",
    "        \n",
    "        print(f\"Text Length Statistics:\")\n",
    "        print(f\"  Characters - Mean: {df_sent['comment_length'].mean():.0f}, Median: {df_sent['comment_length'].median():.0f}\")\n",
    "        print(f\"  Words - Mean: {df_sent['comment_words'].mean():.1f}, Median: {df_sent['comment_words'].median():.0f}\")\n",
    "        \n",
    "        # Label analysis\n",
    "        if 'label' in df_sent.columns:\n",
    "            print(f\"\\nLabel Distribution Analysis:\")\n",
    "            label_counts = df_sent['label'].value_counts()\n",
    "            \n",
    "            for label in label_counts.index:\n",
    "                subset = df_sent[df_sent['label'] == label]\n",
    "                count = len(subset)\n",
    "                percentage = (count / len(df_sent)) * 100\n",
    "                avg_length = subset['comment_words'].mean()\n",
    "                \n",
    "                print(f\"  {label}: {count:,} samples ({percentage:.1f}%) - Avg length: {avg_length:.1f} words\")\n",
    "                \n",
    "                # Sample comment for each label\n",
    "                sample_comment = subset['comment'].iloc[0]\n",
    "                print(f\"    Sample: {sample_comment[:100]}...\")\n",
    "        \n",
    "        # Data quality metrics\n",
    "        print(f\"\\nData Quality Metrics:\")\n",
    "        \n",
    "        # Very short comments (potential quality issues)\n",
    "        short_comments = (df_sent['comment_words'] <= 3).sum()\n",
    "        print(f\"  Very short comments (≤3 words): {short_comments} ({short_comments/len(df_sent)*100:.1f}%)\")\n",
    "        \n",
    "        # Very long comments (potential outliers)\n",
    "        long_comments = (df_sent['comment_words'] >= 100).sum()\n",
    "        print(f\"  Very long comments (≥100 words): {long_comments} ({long_comments/len(df_sent)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c80941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T04:32:11.871071Z",
     "start_time": "2025-09-06T04:32:11.211744Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Data Visualization & Summary\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m12\u001b[39m))\n\u001b[0;32m      4\u001b[0m fig\u001b[38;5;241m.\u001b[39msuptitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVietnamese Text Dataset Analysis\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, fontweight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbold\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Summarization Dataset Visualizations\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Data Visualization & Summary\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Vietnamese Text Dataset Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Summarization Dataset Visualizations\n",
    "if df_summ is not None and 'content_words' in df_summ.columns:\n",
    "    \n",
    "    # 1. Content length distribution\n",
    "    axes[0,0].hist(df_summ['content_words'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0,0].set_title('Content Length Distribution')\n",
    "    axes[0,0].set_xlabel('Words')\n",
    "    axes[0,0].set_ylabel('Frequency')\n",
    "    axes[0,0].axvline(df_summ['content_words'].mean(), color='red', linestyle='--', \n",
    "                     label=f'Mean: {df_summ[\"content_words\"].mean():.0f}')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # 2. Summary length distribution\n",
    "    axes[0,1].hist(df_summ['summary_words'], bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[0,1].set_title('Summary Length Distribution')\n",
    "    axes[0,1].set_xlabel('Words')\n",
    "    axes[0,1].set_ylabel('Frequency')\n",
    "    axes[0,1].axvline(df_summ['summary_words'].mean(), color='red', linestyle='--',\n",
    "                     label=f'Mean: {df_summ[\"summary_words\"].mean():.0f}')\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # 3. Compression ratio distribution\n",
    "    # Filter extreme outliers for better visualization\n",
    "    compression_filtered = df_summ['compression_ratio'][df_summ['compression_ratio'] <= 1.0]\n",
    "    axes[0,2].hist(compression_filtered, bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0,2].set_title('Compression Ratio Distribution')\n",
    "    axes[0,2].set_xlabel('Compression Ratio')\n",
    "    axes[0,2].set_ylabel('Frequency')\n",
    "    axes[0,2].axvline(compression_filtered.mean(), color='red', linestyle='--',\n",
    "                     label=f'Mean: {compression_filtered.mean():.3f}')\n",
    "    axes[0,2].legend()\n",
    "\n",
    "else:\n",
    "    for i in range(3):\n",
    "        axes[0,i].text(0.5, 0.5, 'Summarization\\nData Not Available', \n",
    "                      ha='center', va='center', transform=axes[0,i].transAxes)\n",
    "        axes[0,i].set_title(f'Summarization Plot {i+1}')\n",
    "\n",
    "# Sentiment Dataset Visualizations\n",
    "if df_sent is not None and 'comment_words' in df_sent.columns:\n",
    "    \n",
    "    # 4. Comment length distribution\n",
    "    axes[1,0].hist(df_sent['comment_words'], bins=50, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[1,0].set_title('Comment Length Distribution')\n",
    "    axes[1,0].set_xlabel('Words')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].axvline(df_sent['comment_words'].mean(), color='red', linestyle='--',\n",
    "                     label=f'Mean: {df_sent[\"comment_words\"].mean():.0f}')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # 5. Label distribution\n",
    "    if 'label' in df_sent.columns:\n",
    "        label_counts = df_sent['label'].value_counts()\n",
    "        colors = ['lightcoral', 'lightblue', 'lightgreen'][:len(label_counts)]\n",
    "        \n",
    "        bars = axes[1,1].bar(label_counts.index, label_counts.values, color=colors, edgecolor='black')\n",
    "        axes[1,1].set_title('Sentiment Label Distribution')\n",
    "        axes[1,1].set_xlabel('Sentiment Label')\n",
    "        axes[1,1].set_ylabel('Count')\n",
    "        \n",
    "        # Add percentage labels on bars\n",
    "        for bar, count in zip(bars, label_counts.values):\n",
    "            percentage = (count / len(df_sent)) * 100\n",
    "            axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + len(df_sent)*0.01,\n",
    "                          f'{percentage:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 6. Length by sentiment\n",
    "    if 'label' in df_sent.columns:\n",
    "        sentiment_lengths = []\n",
    "        sentiment_labels = []\n",
    "        \n",
    "        for label in df_sent['label'].unique():\n",
    "            lengths = df_sent[df_sent['label'] == label]['comment_words']\n",
    "            sentiment_lengths.append(lengths)\n",
    "            sentiment_labels.append(label)\n",
    "        \n",
    "        axes[1,2].boxplot(sentiment_lengths, labels=sentiment_labels)\n",
    "        axes[1,2].set_title('Comment Length by Sentiment')\n",
    "        axes[1,2].set_xlabel('Sentiment Label')\n",
    "        axes[1,2].set_ylabel('Words')\n",
    "        \n",
    "else:\n",
    "    for i in range(3):\n",
    "        axes[1,i].text(0.5, 0.5, 'Sentiment\\nData Not Available', \n",
    "                      ha='center', va='center', transform=axes[1,i].transAxes)\n",
    "        axes[1,i].set_title(f'Sentiment Plot {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Final Summary Report\n",
    "\n",
    "print(f\"\\nDataset Overview:\")\n",
    "if df_summ is not None:\n",
    "    print(f\"  Summarization: {len(df_summ):,} samples ready for training\")\n",
    "    if 'compression_ratio' in df_summ.columns:\n",
    "        avg_compression = df_summ['compression_ratio'].mean()\n",
    "        print(f\"average compression: {avg_compression:.1%}\")\n",
    "        print(f\"quality: {'Good' if 0.1 <= avg_compression <= 0.5 else 'Needs Review'}\")\n",
    "else:\n",
    "    print(f\"  Summarization: Dataset not loaded\")\n",
    "\n",
    "if df_sent is not None:\n",
    "    print(f\"  Sentiment: {len(df_sent):,} samples ready for training\")\n",
    "    if 'label' in df_sent.columns:\n",
    "        label_counts = df_sent['label'].value_counts()\n",
    "        imbalance = label_counts.max() / label_counts.min()\n",
    "        print(f\"label balance: {imbalance:.1f}:1 ratio\")\n",
    "        print(f\"quality: {'Balanced' if imbalance <= 3 else 'Imbalanced'}\")\n",
    "else:\n",
    "    print(f\"  Sentiment: Dataset not loaded\")\n",
    "\n",
    "print(f\"\\nConfiguration Status:\")\n",
    "print(f\"  All config files validated\")\n",
    "print(f\"  Data paths confirmed\")\n",
    "print(f\"  Project structure verified\")\n",
    "\n",
    "print(f\"\\nReadiness Assessment:\")\n",
    "ready_count = sum([df_summ is not None, df_sent is not None])\n",
    "print(f\"  - Data readiness: {ready_count}/2 datasets available\")\n",
    "print(f\"  - Status: {'Ready for model training!' if ready_count == 2 else 'Fix data issues before training'}\")\n",
    "\n",
    "if df_summ is not None and 'compression_ratio' in df_summ.columns:\n",
    "    if df_summ['compression_ratio'].mean() > 0.5:\n",
    "        print(\"=\" * 60)\n",
    "if df_sent is not None and 'label' in df_sent.columns:\n",
    "    label_counts = df_sent['label'].value_counts()\n",
    "    if label_counts.max() / label_counts.min() > 3:\n",
    "        print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342b10c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
